{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† RNNs as Computational Dynamical Systems\n",
    "\n",
    "## Tutorial Introduction\n",
    "\n",
    "**Welcome!** In this tutorial, we'll explore recurrent neural networks (RNNs) through the powerful lens of dynamical systems theory. Rather than viewing RNNs simply as sequence processors, we'll understand them as dynamical systems that can be analyzed, visualized, and compared to their biological counterparts.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "1. **RNNs = Dynamical Systems**: How to formulate RNNs as continuous-time ODEs\n",
    "2. **The Lorenz System**: Our benchmark chaotic system for testing\n",
    "3. **Biological Constraints**: E/I balance, Dale's law, spiking neurons\n",
    "4. **Analysis Tools**: Fixed points, Lyapunov exponents, attractor geometry\n",
    "\n",
    "### Tutorial Structure\n",
    "\n",
    "| Notebook | Content | Duration |\n",
    "|----------|---------|----------|\n",
    "| **00 (this)** | Introduction, Lorenz system, setup | 30 min |\n",
    "| **01** | Continuous-Time RNN | 45 min |\n",
    "| **02** | Balanced E/I Rate Network | 45 min |\n",
    "| **03** | Balanced Spiking Network | 45 min |\n",
    "| **04** | Dynamical Systems Analysis | 30 min |\n",
    "| **05** | Synthesis & Comparison | 30 min |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üöÄ Setup\n",
    "\n",
    "First, let's install dependencies and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run this cell on Colab)\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install -q torch torchdiffeq norse matplotlib scipy tqdm\n",
    "    \n",
    "    # Clone the tutorial repository\n",
    "    !git clone -q https://github.com/YOUR_USERNAME/rnn-dynamical-systems-tutorial.git\n",
    "    %cd rnn-dynamical-systems-tutorial\n",
    "    \n",
    "    # Add src to path\n",
    "    sys.path.insert(0, 'src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.integrate import solve_ivp\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìê Part 1: RNNs as Dynamical Systems\n",
    "\n",
    "### The Key Insight\n",
    "\n",
    "A recurrent neural network can be written as a **discrete-time dynamical system**:\n",
    "\n",
    "$$\\mathbf{h}_{t+1} = f(\\mathbf{h}_t, \\mathbf{x}_t; \\theta)$$\n",
    "\n",
    "Or equivalently, as a **continuous-time dynamical system**:\n",
    "\n",
    "$$\\tau \\frac{d\\mathbf{h}}{dt} = -\\mathbf{h} + \\phi(\\mathbf{W}_{rec}\\mathbf{h} + \\mathbf{W}_{in}\\mathbf{x} + \\mathbf{b})$$\n",
    "\n",
    "This formulation lets us use powerful tools from dynamical systems theory:\n",
    "\n",
    "- **Fixed points**: Where $\\frac{d\\mathbf{h}}{dt} = 0$\n",
    "- **Stability analysis**: Eigenvalues of the Jacobian\n",
    "- **Attractors**: Stable patterns the system converges to\n",
    "- **Lyapunov exponents**: Measure of chaos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why This Perspective Matters\n",
    "\n",
    "1. **Interpretability**: We can understand *what* the network has learned in terms of attractor landscapes\n",
    "2. **Comparison to biology**: Neural circuits are continuous-time dynamical systems\n",
    "3. **Universal approximation**: RNNs can approximate any dynamical system (given enough capacity)\n",
    "4. **Analysis tools**: We can find fixed points, bifurcations, and chaos signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ü¶ã Part 2: The Lorenz System ‚Äî Our Benchmark\n",
    "\n",
    "The [Lorenz system](https://en.wikipedia.org/wiki/Lorenz_system) is a classic chaotic dynamical system that will serve as our ground truth throughout this tutorial. It was discovered by Edward Lorenz in 1963 while studying atmospheric convection.\n",
    "\n",
    "### The Equations\n",
    "\n",
    "$$\\frac{dx}{dt} = \\sigma(y - x)$$\n",
    "$$\\frac{dy}{dt} = x(\\rho - z) - y$$\n",
    "$$\\frac{dz}{dt} = xy - \\beta z$$\n",
    "\n",
    "With standard parameters: $\\sigma = 10$, $\\rho = 28$, $\\beta = 8/3$\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "- **Chaotic**: Sensitive dependence on initial conditions\n",
    "- **Strange attractor**: The famous \"butterfly\" shape\n",
    "- **Dissipative**: Volume in phase space contracts\n",
    "- **Low-dimensional**: Only 3 state variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lorenz_system(t, state, sigma=10.0, rho=28.0, beta=8/3):\n",
    "    \"\"\"\n",
    "    Lorenz system ODEs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t : float\n",
    "        Time (not used, but required by ODE solvers)\n",
    "    state : array-like\n",
    "        Current state [x, y, z]\n",
    "    sigma, rho, beta : float\n",
    "        System parameters\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    derivatives : list\n",
    "        [dx/dt, dy/dt, dz/dt]\n",
    "    \"\"\"\n",
    "    x, y, z = state\n",
    "    return [\n",
    "        sigma * (y - x),           # dx/dt\n",
    "        x * (rho - z) - y,         # dy/dt\n",
    "        x * y - beta * z           # dz/dt\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a Lorenz trajectory\n",
    "t_span = (0, 100)  # Time range\n",
    "t_eval = np.linspace(0, 100, 10000)  # Time points to evaluate\n",
    "initial_state = [1.0, 1.0, 1.0]  # Initial condition\n",
    "\n",
    "# Integrate\n",
    "solution = solve_ivp(\n",
    "    lorenz_system, \n",
    "    t_span, \n",
    "    initial_state,\n",
    "    t_eval=t_eval,\n",
    "    method='RK45',\n",
    "    rtol=1e-10,\n",
    "    atol=1e-12\n",
    ")\n",
    "\n",
    "# Extract trajectory\n",
    "t = solution.t\n",
    "trajectory = solution.y.T  # Shape: (n_times, 3)\n",
    "\n",
    "print(f\"Generated trajectory: {trajectory.shape[0]} time points, {trajectory.shape[1]} dimensions\")\n",
    "print(f\"Time step: {t[1] - t[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Lorenz attractor\n",
    "fig = plt.figure(figsize=(14, 5))\n",
    "\n",
    "# 3D view\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot(trajectory[:, 0], trajectory[:, 1], trajectory[:, 2], \n",
    "         lw=0.5, alpha=0.8, color='steelblue')\n",
    "ax1.set_xlabel('X')\n",
    "ax1.set_ylabel('Y')\n",
    "ax1.set_zlabel('Z')\n",
    "ax1.set_title('Lorenz Attractor (3D)')\n",
    "ax1.view_init(elev=20, azim=45)\n",
    "\n",
    "# Time series\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(t[:2000], trajectory[:2000, 0], label='x', alpha=0.8)\n",
    "ax2.plot(t[:2000], trajectory[:2000, 1], label='y', alpha=0.8)\n",
    "ax2.plot(t[:2000], trajectory[:2000, 2], label='z', alpha=0.8)\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.set_ylabel('Value')\n",
    "ax2.set_title('Lorenz Time Series')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Explore: Sensitivity to Initial Conditions\n",
    "\n",
    "A hallmark of chaotic systems is **sensitive dependence on initial conditions** ‚Äî tiny differences grow exponentially. Let's visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two trajectories with slightly different initial conditions\n",
    "ic1 = [1.0, 1.0, 1.0]\n",
    "ic2 = [1.0 + 1e-8, 1.0, 1.0]  # Tiny perturbation!\n",
    "\n",
    "sol1 = solve_ivp(lorenz_system, (0, 50), ic1, t_eval=np.linspace(0, 50, 5000))\n",
    "sol2 = solve_ivp(lorenz_system, (0, 50), ic2, t_eval=np.linspace(0, 50, 5000))\n",
    "\n",
    "# Compute distance between trajectories\n",
    "distance = np.sqrt(np.sum((sol1.y - sol2.y)**2, axis=0))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Trajectories\n",
    "axes[0].plot(sol1.t, sol1.y[0], 'b-', label='Trajectory 1', alpha=0.8)\n",
    "axes[0].plot(sol2.t, sol2.y[0], 'r--', label='Trajectory 2 (perturbed by 1e-8)', alpha=0.8)\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('x')\n",
    "axes[0].set_title('Two Nearly Identical Initial Conditions')\n",
    "axes[0].legend()\n",
    "\n",
    "# Distance (log scale)\n",
    "axes[1].semilogy(sol1.t, distance)\n",
    "axes[1].set_xlabel('Time')\n",
    "axes[1].set_ylabel('Distance (log scale)')\n",
    "axes[1].set_title('Exponential Divergence')\n",
    "axes[1].axhline(1e-8, color='gray', ls='--', label='Initial separation')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial separation: {1e-8:.0e}\")\n",
    "print(f\"Final separation: {distance[-1]:.2f}\")\n",
    "print(f\"Amplification factor: {distance[-1]/1e-8:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Our Task: Lorenz Trajectory Prediction\n",
    "\n",
    "**Goal**: Train an RNN to predict the next state of the Lorenz system given its current state.\n",
    "\n",
    "$$\\hat{\\mathbf{s}}_{t+\\Delta t} = \\text{RNN}(\\mathbf{s}_t)$$\n",
    "\n",
    "where $\\mathbf{s} = [x, y, z]^T$.\n",
    "\n",
    "This is interesting because:\n",
    "1. The dynamics are highly nonlinear\n",
    "2. Long-term prediction is fundamentally limited by chaos\n",
    "3. We can analyze what the trained RNN has learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìä Part 3: Preparing the Data\n",
    "\n",
    "Let's create training, validation, and test sets from our Lorenz trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a longer trajectory for training\n",
    "def generate_lorenz_data(total_time=200, dt=0.01, transient=10, seed=42):\n",
    "    \"\"\"\n",
    "    Generate Lorenz trajectory data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    total_time : float\n",
    "        Total simulation time\n",
    "    dt : float\n",
    "        Time step\n",
    "    transient : float\n",
    "        Time to discard (to reach attractor)\n",
    "    seed : int\n",
    "        Random seed\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    t : np.ndarray\n",
    "        Time points\n",
    "    trajectory : np.ndarray\n",
    "        States, shape (n_times, 3)\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Random initial condition near attractor\n",
    "    ic = [1.0 + np.random.randn()*0.1, \n",
    "          1.0 + np.random.randn()*0.1, \n",
    "          1.0 + np.random.randn()*0.1]\n",
    "    \n",
    "    t_eval = np.arange(0, total_time + transient, dt)\n",
    "    \n",
    "    sol = solve_ivp(\n",
    "        lorenz_system,\n",
    "        (0, total_time + transient),\n",
    "        ic,\n",
    "        t_eval=t_eval,\n",
    "        method='RK45',\n",
    "        rtol=1e-10\n",
    "    )\n",
    "    \n",
    "    # Remove transient\n",
    "    transient_steps = int(transient / dt)\n",
    "    t = sol.t[transient_steps:] - transient\n",
    "    trajectory = sol.y[:, transient_steps:].T\n",
    "    \n",
    "    return t, trajectory\n",
    "\n",
    "# Generate data\n",
    "t_data, trajectory_data = generate_lorenz_data(total_time=200, dt=0.01)\n",
    "print(f\"Generated {len(t_data)} time steps ({t_data[-1]:.0f} time units)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/val/test\n",
    "n_total = len(trajectory_data)\n",
    "n_train = int(0.7 * n_total)\n",
    "n_val = int(0.15 * n_total)\n",
    "n_test = n_total - n_train - n_val\n",
    "\n",
    "train_data = trajectory_data[:n_train]\n",
    "val_data = trajectory_data[n_train:n_train+n_val]\n",
    "test_data = trajectory_data[n_train+n_val:]\n",
    "\n",
    "print(f\"Train: {len(train_data)} steps ({len(train_data)*0.01:.0f} time units)\")\n",
    "print(f\"Val: {len(val_data)} steps ({len(val_data)*0.01:.0f} time units)\")\n",
    "print(f\"Test: {len(test_data)} steps ({len(test_data)*0.01:.0f} time units)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data (important for training!)\n",
    "train_mean = train_data.mean(axis=0)\n",
    "train_std = train_data.std(axis=0)\n",
    "\n",
    "train_normalized = (train_data - train_mean) / train_std\n",
    "val_normalized = (val_data - train_mean) / train_std\n",
    "test_normalized = (test_data - train_mean) / train_std\n",
    "\n",
    "print(f\"Normalization stats:\")\n",
    "print(f\"  Mean: {train_mean}\")\n",
    "print(f\"  Std: {train_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LorenzDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for Lorenz prediction task.\n",
    "    \n",
    "    Given a sequence of states, predict the next state.\n",
    "    \"\"\"\n",
    "    def __init__(self, trajectory, seq_length=50):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        trajectory : np.ndarray\n",
    "            Shape (n_times, 3)\n",
    "        seq_length : int\n",
    "            Input sequence length\n",
    "        \"\"\"\n",
    "        self.trajectory = torch.tensor(trajectory, dtype=torch.float32)\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.trajectory) - self.seq_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Input: sequence of states\n",
    "        x = self.trajectory[idx:idx+self.seq_length]\n",
    "        # Target: next state\n",
    "        y = self.trajectory[idx+self.seq_length]\n",
    "        return x, y\n",
    "\n",
    "# Create datasets\n",
    "seq_length = 50  # Use 50 time steps as input\n",
    "\n",
    "train_dataset = LorenzDataset(train_normalized, seq_length)\n",
    "val_dataset = LorenzDataset(val_normalized, seq_length)\n",
    "test_dataset = LorenzDataset(test_normalized, seq_length)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)} samples\")\n",
    "print(f\"  Val: {len(val_dataset)} samples\")\n",
    "print(f\"  Test: {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample\n",
    "sample_x, sample_y = train_dataset[0]\n",
    "print(f\"Input shape: {sample_x.shape}\")\n",
    "print(f\"Target shape: {sample_y.shape}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "\n",
    "for i, (ax, dim) in enumerate(zip(axes, ['x', 'y', 'z'])):\n",
    "    ax.plot(range(seq_length), sample_x[:, i].numpy(), 'b-', label='Input')\n",
    "    ax.scatter([seq_length], [sample_y[i].numpy()], c='red', s=100, zorder=5, label='Target')\n",
    "    ax.set_xlabel('Time step')\n",
    "    ax.set_ylabel(dim)\n",
    "    ax.legend()\n",
    "    ax.set_title(f'Dimension: {dim}')\n",
    "\n",
    "plt.suptitle('Sample: Predict next state from sequence', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß™ Part 4: Preview ‚Äî The Networks We'll Build\n",
    "\n",
    "Over the next notebooks, we'll implement and compare three network architectures:\n",
    "\n",
    "### 1. Continuous-Time RNN (CT-RNN)\n",
    "$$\\tau \\frac{d\\mathbf{h}}{dt} = -\\mathbf{h} + \\tanh(\\mathbf{W}_{rec}\\mathbf{h} + \\mathbf{W}_{in}\\mathbf{x} + \\mathbf{b})$$\n",
    "\n",
    "- Smooth dynamics\n",
    "- Amenable to ODE analysis\n",
    "- Trained using Neural ODEs (adjoint method)\n",
    "\n",
    "### 2. Balanced E/I Rate Network\n",
    "$$\\tau_E \\frac{d\\mathbf{r}_E}{dt} = -\\mathbf{r}_E + \\phi(\\mathbf{W}_{EE}\\mathbf{r}_E - \\mathbf{W}_{EI}\\mathbf{r}_I + \\mathbf{I}_{ext})$$\n",
    "$$\\tau_I \\frac{d\\mathbf{r}_I}{dt} = -\\mathbf{r}_I + \\phi(\\mathbf{W}_{IE}\\mathbf{r}_E - \\mathbf{W}_{II}\\mathbf{r}_I)$$\n",
    "\n",
    "- Separate excitatory (E) and inhibitory (I) populations\n",
    "- Dale's law: E neurons have positive weights, I neurons have negative\n",
    "- Dynamically balanced regime\n",
    "\n",
    "### 3. Balanced Spiking Network\n",
    "$$\\tau_m \\frac{dV}{dt} = -(V - V_{rest}) + I_{syn} + I_{ext}$$\n",
    "$$\\text{if } V > V_{th}: \\text{spike}, V \\rightarrow V_{reset}$$\n",
    "\n",
    "- Leaky Integrate-and-Fire (LIF) neurons\n",
    "- Event-driven communication (spikes)\n",
    "- Reservoir computing approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Summary\n",
    "\n",
    "In this introduction, we've:\n",
    "\n",
    "1. ‚úÖ Set up our environment and imports\n",
    "2. ‚úÖ Understood RNNs as dynamical systems\n",
    "3. ‚úÖ Explored the Lorenz system ‚Äî our benchmark\n",
    "4. ‚úÖ Prepared training, validation, and test data\n",
    "5. ‚úÖ Previewed the three network architectures\n",
    "\n",
    "### Next: Notebook 01 ‚Äî Continuous-Time RNN\n",
    "\n",
    "We'll implement a CT-RNN using Neural ODEs and train it on the Lorenz prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üíæ Save Data for Later Notebooks\n",
    "\n",
    "Let's save the preprocessed data so we can use it across notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "import os\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "np.savez(\n",
    "    'data/processed/lorenz_data.npz',\n",
    "    train=train_normalized,\n",
    "    val=val_normalized,\n",
    "    test=test_normalized,\n",
    "    mean=train_mean,\n",
    "    std=train_std,\n",
    "    dt=0.01,\n",
    "    seq_length=seq_length\n",
    ")\n",
    "\n",
    "print(\"Data saved to data/processed/lorenz_data.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to load data in other notebooks\n",
    "def load_lorenz_data(path='data/processed/lorenz_data.npz'):\n",
    "    \"\"\"\n",
    "    Load preprocessed Lorenz data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    data : dict\n",
    "        Contains train, val, test arrays and normalization params\n",
    "    \"\"\"\n",
    "    data = np.load(path)\n",
    "    return {\n",
    "        'train': data['train'],\n",
    "        'val': data['val'],\n",
    "        'test': data['test'],\n",
    "        'mean': data['mean'],\n",
    "        'std': data['std'],\n",
    "        'dt': float(data['dt']),\n",
    "        'seq_length': int(data['seq_length'])\n",
    "    }\n",
    "\n",
    "# Test loading\n",
    "loaded = load_lorenz_data()\n",
    "print(f\"Loaded data shapes: train={loaded['train'].shape}, val={loaded['val'].shape}, test={loaded['test'].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
