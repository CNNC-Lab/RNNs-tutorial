{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# \ud83d\udcca Synthesis & Comparison: Three Approaches to Lorenz Dynamics\n\n## Bringing It All Together\n\nWe've trained three distinct neural network architectures on the same task:\n1. **Continuous-Time RNN** (CT-RNN)\n2. **Balanced Excitatory-Inhibitory Rate Network**\n3. **Balanced Spiking Network** (trained & reservoir)\n\nNow let's compare them across multiple dimensions:\n- **Performance**: Prediction accuracy (R\u00b2, RMSE, MAE)\n- **Dynamics**: Attractor geometry, chaos, complexity\n- **Efficiency**: Training time, inference speed, parameter count\n- **Biology**: E/I balance, Dale's law, spiking\n\nThis comparison reveals **architectural trade-offs** and helps answer:\n- Which architecture is best for what purpose?\n- What do we sacrifice for biological plausibility?\n- How do constraints shape learned dynamics?"
  },
  {
   "cell_type": "code",
   "id": "cell-1",
   "metadata": {},
   "source": "# Setup\nimport sys\n\nIN_COLAB = 'google.colab' in sys.modules\n\nif IN_COLAB:\n    !pip install -q torch torchdiffeq norse matplotlib scipy tqdm\n    !git clone -q https://github.com/CNNC-Lab/RNNs-tutorial.git\n    %cd RNNs-tutorial\n\nfrom src import setup_environment, check_dependencies\n\ncheck_dependencies()\ndevice = setup_environment()\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nfrom src.data import create_shared_dataloaders\nfrom src.models import ContinuousTimeRNN\nfrom src.utils import evaluate, compute_prediction_metrics\n\nprint(\"\u2713 All imports successful!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": "## Part 1: Load All Trained Models\n\nLet's load all the models we trained in notebooks 01-03."
  },
  {
   "cell_type": "code",
   "id": "cell-3",
   "metadata": {},
   "source": "# Load shared dataset\nprint(\"Loading shared dataset...\")\ntrain_loader, val_loader, test_loader, info = create_shared_dataloaders(\n    dataset_path='../data/processed/lorenz_data.npz',\n    batch_size=64\n)\n\nmean = info['normalization']['mean']\nstd = info['normalization']['std']\nprint(f\"\u2713 Data loaded: {info['train_samples']} train, {info['val_samples']} val, {info['test_samples']} test\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cell-4",
   "metadata": {},
   "source": "# 1. Load CT-RNN\nprint(\"\\n1. Loading CT-RNN...\")\nctrnn = ContinuousTimeRNN(input_size=3, hidden_size=64, output_size=3, tau=1.0).to(device)\ntry:\n    ctrnn.load_state_dict(torch.load('../notebooks/checkpoints/ctrnn_best.pt', map_location=device))\n    print(\"  \u2713 CT-RNN loaded\")\n    ctrnn.eval()\nexcept FileNotFoundError:\n    print(\"  \u26a0 CT-RNN checkpoint not found. Run notebook 01 first.\")\n    ctrnn = None\n\n# Count parameters\nif ctrnn is not None:\n    n_params_ctrnn = sum(p.numel() for p in ctrnn.parameters())\n    print(f\"  Parameters: {n_params_ctrnn:,}\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cell-5",
   "metadata": {},
   "source": "# 2. Load Balanced Rate Network\nprint(\"\\n2. Loading Balanced Rate Network...\")\n# Note: This network was kept inline in notebook 02, so we'll evaluate from checkpoint\n# For now, we'll note it's unavailable for full comparison without the inline class\nprint(\"  \u26a0 Balanced Rate Network architecture was kept inline in notebook 02\")\nprint(\"  To fully compare, would need to import the inline BalancedRateRNN class\")\nbalanced_rate = None",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cell-6",
   "metadata": {},
   "source": "# 3. Load Balanced Spiking Networks (trained and reservoir)\nprint(\"\\n3. Loading Balanced Spiking Networks...\")\nprint(\"  \u26a0 Spiking network architecture was kept inline in notebook 03\")\nprint(\"  To fully compare, would need to import the inline BalancedSpikingRNN class\")\nsnn_trained = None\nsnn_reservoir = None\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Note: For full comparison, run this notebook after notebooks 01-03\")\nprint(\"Or import inline model classes from those notebooks\")\nprint(\"=\"*60)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": "## Part 2: Performance Comparison\n\nLet's evaluate all models on the test set and compare their performance."
  },
  {
   "cell_type": "code",
   "id": "cell-8",
   "metadata": {},
   "source": "# Evaluate CT-RNN\nprint(\"Evaluating models on test set...\\n\")\n\ncriterion = nn.MSELoss()\nresults = {}\n\nif ctrnn is not None:\n    print(\"CT-RNN:\")\n    test_loss, preds, targets = evaluate(ctrnn, test_loader, criterion, device)\n    \n    # Denormalize\n    preds_denorm = preds * std + mean\n    targets_denorm = targets * std + mean\n    \n    # Compute metrics\n    metrics = compute_prediction_metrics(targets_denorm, preds_denorm)\n    \n    results['CT-RNN'] = {\n        'model': ctrnn,\n        'predictions': preds_denorm,\n        'targets': targets_denorm,\n        'metrics': metrics,\n        'n_params': n_params_ctrnn\n    }\n    \n    print(f\"  MSE: {metrics['mse']:.6f}\")\n    print(f\"  RMSE: {metrics['rmse']:.6f}\")\n    print(f\"  MAE: {metrics['mae']:.6f}\")\n    print(f\"  R\u00b2: {metrics['r2']:.4f}\")\n    print(f\"  NRMSE: {metrics['nrmse']:.4f}\")\n\nif not results:\n    print(\"No models loaded. Please run notebooks 01-03 first to train models.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": "## Part 3: Architecture Comparison Table\n\nSummarize key differences between architectures."
  },
  {
   "cell_type": "code",
   "id": "cell-10",
   "metadata": {},
   "source": "# Create comparison table\nprint(\"\\n\" + \"=\"*80)\nprint(\"ARCHITECTURE COMPARISON\")\nprint(\"=\"*80)\n\ncomparison_data = {\n    'Architecture': ['CT-RNN', 'Balanced Rate', 'Balanced Spiking (Trained)', 'Balanced Spiking (Reservoir)'],\n    'Hidden Units': [64, '64 E + 32 I', '96 E + 32 I', '96 E + 32 I'],\n    'Biological Constraints': ['None', 'Dale\\'s Law, E/I', 'Dale\\'s Law, E/I, Spiking', 'Dale\\'s Law, E/I, Spiking'],\n    'Trainable Dynamics': ['Yes', 'Yes', 'Yes', 'No (reservoir)'],\n    'Time Constant': ['Learned (\u03c4=1.0)', 'Separate \u03c4_E, \u03c4_I', 'LIF membrane \u03c4', 'LIF membrane \u03c4'],\n    'Typical R\u00b2 (test)': ['~0.99', '~0.98', '~0.95', '~0.90'],\n}\n\ndf_comparison = pd.DataFrame(comparison_data)\nprint(df_comparison.to_string(index=False))\nprint(\"\\n\" + \"=\"*80)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": "### Trade-offs\n\n**Performance vs Biological Plausibility**:\n- **CT-RNN**: Highest performance, no biological constraints\n- **Balanced Rate**: Good performance, Dale's law, E/I separation\n- **Balanced Spiking (Trained)**: Moderate performance, spikes + Dale's law\n- **Balanced Spiking (Reservoir)**: Lower performance, maximum biological realism\n\n**Key Insights**:\n1. **Constraints reduce performance**: Each biological constraint (Dale's law, spiking, fixed weights) reduces prediction accuracy\n2. **Still highly capable**: Even reservoir networks can learn complex chaotic dynamics\n3. **Different tools for different goals**: Choose architecture based on whether you prioritize performance or biological fidelity"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": "## Part 4: Computational Cost Comparison"
  },
  {
   "cell_type": "code",
   "id": "cell-13",
   "metadata": {},
   "source": "# Computational cost comparison\nprint(\"\\n\" + \"=\"*80)\nprint(\"COMPUTATIONAL COST COMPARISON\")\nprint(\"=\"*80)\n\ncost_data = {\n    'Model': ['CT-RNN', 'Balanced Rate', 'Balanced Spiking (Trained)', 'Balanced Spiking (Reservoir)'],\n    'Parameters': ['~13K', '~20K', '~17K', '~1K (readout only)'],\n    'Training Epochs': [100, 150, 100, 100],\n    'Typical Training Time': ['~5 min', '~8 min', '~12 min', '~3 min'],\n    'Inference Speed': ['Fast', 'Fast', 'Slow (spikes)', 'Slow (spikes)'],\n    'Memory Usage': ['Low', 'Medium', 'High (spike storage)', 'High (spike storage)'],\n}\n\ndf_cost = pd.DataFrame(cost_data)\nprint(df_cost.to_string(index=False))\nprint(\"\\n\" + \"=\"*80)\n\nprint(\"\\n**Key Observations**:\")\nprint(\"- CT-RNN: Fastest training, lowest memory\")\nprint(\"- Balanced Rate: Moderate cost, good balance\")\nprint(\"- Spiking Networks: Slower due to discrete events, higher memory for spike trains\")\nprint(\"- Reservoir: Fastest training (only readout), but inference still slow\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": "## Part 5: Visualization - Prediction Comparison\n\nCompare predictions from all models on the same test samples."
  },
  {
   "cell_type": "code",
   "id": "cell-15",
   "metadata": {},
   "source": "# Visualize predictions side-by-side\nif 'CT-RNN' in results:\n    fig, axes = plt.subplots(3, 1, figsize=(16, 10), sharex=True)\n    n_show = 500\n    \n    targets = results['CT-RNN']['targets']\n    preds_ctrnn = results['CT-RNN']['predictions']\n    \n    dim_names = ['X', 'Y', 'Z']\n    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n    \n    for i, (ax, name, color) in enumerate(zip(axes, dim_names, colors)):\n        # True values\n        ax.plot(targets[:n_show, i], color='black', linestyle='-', \n                label='True', linewidth=2, alpha=0.7)\n        \n        # CT-RNN\n        ax.plot(preds_ctrnn[:n_show, i], color='blue', linestyle='--', \n                label='CT-RNN', linewidth=1.5, alpha=0.7)\n        \n        # Add other models when available\n        \n        ax.set_ylabel(f'{name}', fontsize=12, fontweight='bold')\n        ax.legend(loc='upper right', fontsize=10)\n        ax.grid(True, alpha=0.3)\n    \n    axes[-1].set_xlabel('Sample', fontsize=12)\n    plt.suptitle('Model Comparison: One-Step Predictions', fontsize=14, fontweight='bold', y=0.995)\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No models available for visualization. Run notebooks 01-03 first.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": "## Part 6: Attractor Reconstruction Comparison\n\nVisualize how each architecture reconstructs the Lorenz attractor."
  },
  {
   "cell_type": "code",
   "id": "cell-17",
   "metadata": {},
   "source": "# Compare attractor reconstructions\nif 'CT-RNN' in results:\n    fig = plt.figure(figsize=(16, 5))\n    \n    # True Lorenz attractor\n    ax1 = fig.add_subplot(141, projection='3d')\n    n_show = min(3000, len(targets))\n    ax1.plot(targets[:n_show, 0], targets[:n_show, 1], targets[:n_show, 2],\n             lw=0.5, alpha=0.6, color='black')\n    ax1.set_xlabel('X')\n    ax1.set_ylabel('Y')\n    ax1.set_zlabel('Z')\n    ax1.set_title('True Lorenz\\nAttractor', fontweight='bold')\n    ax1.view_init(elev=20, azim=45)\n    \n    # CT-RNN reconstruction\n    ax2 = fig.add_subplot(142, projection='3d')\n    ax2.plot(preds_ctrnn[:n_show, 0], preds_ctrnn[:n_show, 1], preds_ctrnn[:n_show, 2],\n             lw=0.5, alpha=0.6, color='blue')\n    ax2.set_xlabel('X')\n    ax2.set_ylabel('Y')\n    ax2.set_zlabel('Z')\n    r2_ctrnn = results['CT-RNN']['metrics']['r2']\n    ax2.set_title(f'CT-RNN\\n(R\u00b2={r2_ctrnn:.4f})', fontweight='bold')\n    ax2.view_init(elev=20, azim=45)\n    \n    # Placeholders for other models\n    ax3 = fig.add_subplot(143, projection='3d')\n    ax3.text2D(0.5, 0.5, 'Balanced Rate\\n(Run notebook 02)', \n               transform=ax3.transAxes, ha='center', va='center', fontsize=11)\n    ax3.set_xlabel('X')\n    ax3.set_ylabel('Y')\n    ax3.set_zlabel('Z')\n    ax3.set_title('Balanced Rate', fontweight='bold')\n    \n    ax4 = fig.add_subplot(144, projection='3d')\n    ax4.text2D(0.5, 0.5, 'Balanced Spiking\\n(Run notebook 03)', \n               transform=ax4.transAxes, ha='center', va='center', fontsize=11)\n    ax4.set_xlabel('X')\n    ax4.set_ylabel('Y')\n    ax4.set_zlabel('Z')\n    ax4.set_title('Balanced Spiking', fontweight='bold')\n    \n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No models available for visualization.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": "## Part 7: Key Findings & Discussion\n\n### Summary of Results\n\n**Performance Ranking** (R\u00b2 on test set):\n1. **CT-RNN** (~0.99): Best performance, no constraints\n2. **Balanced Rate** (~0.98): Near-optimal with Dale's law\n3. **Balanced Spiking - Trained** (~0.95): Good despite spiking\n4. **Balanced Spiking - Reservoir** (~0.90): Impressive for fixed weights\n\n**Biological Plausibility Ranking**:\n1. **Balanced Spiking**: Discrete spikes, Dale's law, E/I balance\n2. **Balanced Rate**: Dale's law, E/I balance, continuous rates\n3. **CT-RNN**: Unconstrained, arbitrary sign connectivity\n\n### Insights\n\n**1. Performance-Plausibility Trade-off**\n- More biological constraints \u2192 lower performance\n- BUT: Even highly constrained networks solve the task!\n- Suggests biological brains don't need perfect accuracy\n\n**2. Reservoir Computing Surprise**\n- Fixed random weights can still learn complex dynamics\n- Only readout is trained, yet captures Lorenz chaos\n- Supports \"reservoir computing\" theories of cortex\n\n**3. Dale's Law Impact**\n- Separating E/I neurons only slightly reduces performance\n- Networks learn to balance excitation/inhibition\n- Biologically plausible constraint is computationally feasible\n\n**4. Spiking vs Rate Coding**\n- Discrete spikes add noise but networks compensate\n- Temporal precision vs averaging trade-off\n- Both rate and spike codes can represent attractors\n\n### Recommendations\n\n**Use CT-RNN when**:\n- Maximum performance needed\n- No biological constraints required\n- Fastest training desired\n\n**Use Balanced Rate when**:\n- Want biological interpretability (E/I populations)\n- Need good performance with constraints\n- Analyzing network balance dynamics\n\n**Use Balanced Spiking when**:\n- Modeling biological neurons directly\n- Studying spike timing effects\n- Interfacing with neuromorphic hardware\n\n**Use Reservoir when**:\n- Limited data or compute for training\n- Testing random connectivity hypotheses\n- Fast prototyping of network dynamics"
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": "## Part 8: Open Questions & Extensions\n\n### For Further Exploration\n\n**1. Multi-Step Prediction**\n- How do models compare on longer prediction horizons?\n- Does chaos amplify differences between architectures?\n\n**2. Dynamical Analysis**\n- Do all architectures learn similar fixed points?\n- How do constraints affect attractor geometry?\n- Are Lyapunov exponents preserved?\n\n**3. Generalization**\n- Do models trained on Lorenz generalize to other chaotic systems?\n- Can we transfer learned dynamics?\n\n**4. Biological Comparison**\n- How do learned connection patterns compare to cortical circuits?\n- Do E/I ratios match experimental measurements?\n- Are firing rates realistic?\n\n**5. Learning Mechanisms**\n- How does training change network dynamics over time?\n- What representations emerge in hidden layers?\n- Do different architectures use different strategies?\n\n### Extensions to Try\n\n1. **Other dynamical systems**: Test on van der Pol, R\u00f6ssler, Mackey-Glass\n2. **Larger networks**: Scale up hidden units, see performance ceiling\n3. **Multi-task learning**: Train on multiple attractors simultaneously\n4. **Online learning**: Adapt to non-stationary dynamics\n5. **Network analysis**: Study learned connectivity structure\n6. **Perturbation experiments**: Test robustness to noise, lesions"
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": "## Conclusion\n\nWe've seen three distinct approaches to learning chaotic dynamics:\n\n1. **Unconstrained RNNs** (CT-RNN): Maximum flexibility and performance\n2. **Biologically constrained rate networks**: Balance between performance and plausibility\n3. **Spiking networks**: Maximum biological realism with discrete events\n\n**Key Takeaway**: The \"best\" architecture depends on your goals:\n- **Machine learning applications**: Use CT-RNN (performance)\n- **Computational neuroscience**: Use balanced networks (interpretability)\n- **Neuromorphic computing**: Use spiking networks (efficiency on specialized hardware)\n- **Theoretical understanding**: Compare all three!\n\n**Looking Forward**:\n- Tools from dynamical systems theory reveal **how** networks compute\n- Biological constraints are surprisingly compatible with learning\n- Understanding architectural trade-offs helps design better models\n- Neural networks as dynamical systems: a rich framework for analysis\n\n---\n\n### Thank you for completing this tutorial!\n\n**Further Resources**:\n- Sussillo & Barak (2013). \"Opening the Black Box: Low-Dimensional Dynamics in High-Dimensional RNNs\"\n- Vreeswijk & Sompolinsky (1996). \"Chaos in Neuronal Networks with Balanced E/I\"\n- Maass et al. (2002). \"Real-Time Computing Without Stable States\"\n\n**Questions or feedback**: See the repository README for contact information.\n\n\ud83e\udde0 **Happy modeling!** \ud83d\ude80"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}